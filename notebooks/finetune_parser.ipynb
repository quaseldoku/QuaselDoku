{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d350aea4",
   "metadata": {},
   "source": [
    "# Finetuning des Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b18c7",
   "metadata": {},
   "source": [
    "Hier soll eine Möglichkeit erarbeitet werden, mit der die HTML-Files am besten extrahiert und bereinigt werden können. Damit soll eine reibungslose Weiterverarbeitung der Texte ermöglicht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1ec6a",
   "metadata": {},
   "source": [
    "## Vorüberlegung:\n",
    "\n",
    "Für unser Projekt sind verschiedene Teile des HTML-Files besonders von Bedeutung:\n",
    "\n",
    "- Text: Inhalt der HTML-Seite\n",
    "    - Diese sollte wenn möglich in einzelne Abschnitte gegliedert werden (ermöglicht später die exakte Beantwortung von Fragen)\n",
    "    - Unwichtige Elemente der Seite (wie Menü-Punkte, die auf jeder Seite erscheinen) gilt es zu filtern\n",
    "    - Nichtdarstellbare Zeichen gilt es zu filtern\n",
    "- Überschrift und Unterüberschrift der Paragraphen\n",
    "    - Wenn man im späteren Verlauf auf eine Frage einen Paragrafen referenzieren möchte, ist es wichtig die zum Paragraph gehörende Unterüberschrift und die Verlinkung dahin zu vermerken\n",
    "- HTML-File: Der Link zum verwendeten File sollte als Meta-Data vermerkt werden um später zur Seite navigieren zu können\n",
    "    - (vielleicht kann man noch den Link zur genauen Teil-Überschrift extrahieren)\n",
    "- (OPTIONAL Links und Bilder: Man könnte die verwendeten Links (href) und den Pfad zu den Bildern mit extrahieren, stellt sich aber die Frage, in wie weit das relevant ist) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa810f3",
   "metadata": {},
   "source": [
    "### Benötigte Pakete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d131563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import itertools\n",
    "import base64\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6ea83",
   "metadata": {},
   "source": [
    "(Ich habe Probleme in kedro jupyter notebooks das Paket pandas zu verwenden, mit diesem workaround ist es mir aber möglich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24517f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66da620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2be45",
   "metadata": {},
   "source": [
    "### Auslesen von HTML-Elementen\n",
    "\n",
    "Hier wird zunächst nur repräsentativ ein kleiner Teil der Dokumentationsdateien ausgelesen (alle Seiten zum Punkt \"Bedienung\").\n",
    "Die ausgelesenen Daten werden in einem CSV-File gespeichert:\n",
    "\n",
    "|Title| Text| Source |\n",
    "|:----|:----|:-------|\n",
    "| h1  |...  | file1  |\n",
    "| h2  |...  | file1  |\n",
    "| h1  |...  | file2  |\n",
    "| h2  |...  | file2  |\n",
    "| h2  |...  | file2  |\n",
    "| ... |...  | ...    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db1eae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'path/to/*.html'\n",
    "files = glob.glob(path)\n",
    "all_data = []\n",
    "for file in files:\n",
    "    f = open(file, 'r', encoding = \"utf-8\")\n",
    "    name = file\n",
    "    soup = BeautifulSoup(f.read(),'html.parser')\n",
    "    h1 = get_body(soup, get_paraName(soup,\"h1\"),name)\n",
    "    h2 = get_body(soup, get_paraName(soup,\"h2\"),name)\n",
    "    all_data.append(h1)\n",
    "    all_data.append(h2)\n",
    "    f.close()\n",
    "    #write the data\n",
    "flat_list = list(itertools.chain(*all_data))\n",
    "\n",
    "dataframe = pd.DataFrame(flat_list, columns = [\"Title\",\"Source\",\"Hash\",\"Text\",\"Type\"])\n",
    "#dataframe = dataframe.astype({\"Text\": list})\n",
    "dataframe.to_csv(\"test_cleanFiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0090030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Type\n",
      "0  Zur Konfiguration von ECU-TEST gehören im Wese...    p\n",
      "1          Sind die Konfigurationen geladen, können:    p\n",
      "2  Die beschriebene Möglichkeit zum Starten von K...    p\n",
      "3  Die im Konfigurationswechselschritt angegebene...    p\n",
      "4  Bei der Testbenchkonfiguration (kurz TBC) werd...    p\n"
     ]
    }
   ],
   "source": [
    "print(dataframe[[\"Text\",\"Type\"]][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b6dd0",
   "metadata": {},
   "source": [
    "#### Erzeugen des Text-Body Elements\n",
    "\n",
    "Im Div-Block mit der Klasse \"document\" wird der Inhalt der Seite umschlossen (ausgeschlossen das Seiten Menü, Header und Fußzeile)\n",
    "\n",
    "Die Unterteilungen in der Seite erfolgen durch kleinere Div-Blöcke der Klasse \"section\".\n",
    "Die id der Div-Blöcke entspricht der Überschrift (h1 oder h2).\n",
    "Daher werden nun Anhand der Überschriften zunächst die Div-Blöcke gesucht und anschließend die p-Blöcke, Auflistungen (ul) und Tabellen (table) ausgelesen.\n",
    "\n",
    "Zudem wird noch ein eindeutiger Hashwert auf Basis der aktuellen Überschrift erstell, um später die Elemente eindeutig ausweisen zu können.\n",
    "\n",
    "**Neuste Änderung**\n",
    "\n",
    "HTML-Elemente werden nun einzeln ausgelsen. Die so erzeugten Strings werden als individuelle Zeilen gespeichert (mit Referenz auf Paragraph und Dokument. Bei Bedarf können diese wieder zu einem Paragraph oder Dokument zusammen gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f514b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(soup_elem,liste_headlines,name):\n",
    "    paragraphs = []\n",
    "\n",
    "    for h in liste_headlines:\n",
    "\n",
    "        # find div element that belongs to this h-Element\n",
    "        block = soup_elem.find(\"div\", {\"id\": h.lower()})\n",
    "        \n",
    "        # re assemble filepath within doku folder structure\n",
    "        file_path = name.replace('_', '/')\n",
    "            \n",
    "        # find all child elements of this div block\n",
    "        child_tags = block.find((\"p\", \"ul\", \"table\"), recursive=False)\n",
    "\n",
    "        # gather all availabe text within this div block and concatenate\n",
    "        text_elements = []\n",
    "\n",
    "        if child_tags != None:\n",
    "\n",
    "            for child in (block.findChildren((\"p\",\"ul\", \"table\"), recursive=False)):\n",
    "                        \n",
    "                text = re.sub(\" \\n \", \" \", child.get_text().strip(), flags=re.M)\n",
    "                text = re.sub(\"\\n \", \" \", text.strip(), flags=re.M)\n",
    "                text = re.sub(\" \\n\", \" \", text.strip(), flags=re.M)\n",
    "                text = re.sub(\"\\n\", \" \", text.strip(), flags=re.M)\n",
    "                if len(text) > 0:\n",
    "                    # concat all text elements to generate hash\n",
    "                    _text_concat = \" \".join(text_elements)\n",
    "                    signature = generate_hash_from_text(_text_concat)\n",
    "\n",
    "            \n",
    "                paragraphs.append([signature, h, file_path, text, child.name])\n",
    "                #text_elements.append(text)\n",
    "            \n",
    "        \n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0536d9",
   "metadata": {},
   "source": [
    "## NEU: Hash-Code getrennt generieren\n",
    "\n",
    "nach md5 Standard (Made by Martin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102469c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash_from_text(text):\n",
    "    return base64.b64encode(hashlib.md5(text.encode('utf-8')).digest()).decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b18cc",
   "metadata": {},
   "source": [
    "#### Erzeugen des Text-Body Elements (VERALTET)\n",
    "\n",
    "Aktuell wird der Teil für h1-Überschriften getrennt betrachtet, da hier spezielle Parent-children Bezeihungen verwendet werden\n",
    "um den Text der nur zur h1-Überschrift gehört zu extrahieren.\n",
    "\n",
    "Da die Funktion get_body() zur korrekten Teilung der h2 Überschriften angepasst werden musste, ist diese Funktion nun hinfällig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body_h1(soup_elem,liste_headlines ,name):\n",
    "    clean_para=[]\n",
    "    ordnen = []\n",
    "    for h1 in liste_headlines:\n",
    "        block = soup_elem.find(\"div\",{\"id\":h1.lower()})\n",
    "        for para in block.findChildren((\"p\",\"ul\"), recursive=False):\n",
    "            clean_para.append(re.sub(\"\\n\", \" \", para.get_text().strip(), flags=re.M))\n",
    "        ordnen.append((h1, clean_para,name))\n",
    "    return ordnen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61018b8a",
   "metadata": {},
   "source": [
    "#### Finde Kapitel\n",
    "\n",
    "Es müssen die Kapitelnamen und die Namen der Unterkapitel für jede Datei gesucht werden. Nur so kann später der Inhalt den Überschriften zugeordnet werden.\n",
    "Ein zu der Überschrift gehörender Block, trägt meist den Namen der dazugehördenen Überschrift. Ausnahmen entstehen durch Sonderzeichen.\n",
    "Da die h1-Überschriften anders zu behandeln sind als h2-Überschriften muss als Parameter übergeben werden welcher Typ von Überschrift (headline \"h1\" oder \"h2\") gerade betrachtet wird.\n",
    "\n",
    "Überlegung: Zur Zeit werden die Namen der Überschriften noch umständlich angepasst, um als id der zugehörigen Div-Blöcke verwendet zu werden. Vielleicht gibt es da einen besseren Work-Around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5525a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: DeprecationWarning: invalid escape sequence '\\W'\n",
      "<>:9: DeprecationWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_20080\\515517086.py:9: DeprecationWarning: invalid escape sequence '\\W'\n",
      "  clean_text = re.sub('\\W+', ' ', clean_text)\n"
     ]
    }
   ],
   "source": [
    "def get_paraName(soup_elem, headline):\n",
    "    all_chap = []\n",
    "    for unterkap in soup_elem.find_all(headline):\n",
    "        clean_text = re.sub('\\uf0c1', '', unterkap.get_text())\n",
    "        clean_text = clean_text.replace(\"(\", \" \").replace(\")\", \"\")\n",
    "        clean_text = clean_text.replace(\"„\", \" \").replace(\"“\", \"\")\n",
    "        clean_text = clean_text.replace(\"?\", \"\")\n",
    "        clean_text = clean_text.replace(\"_\", \" \")\n",
    "        clean_text = re.sub('\\W+', ' ', clean_text)\n",
    "        clean_text = re.sub('ü', 'u', clean_text)\n",
    "        clean_text = re.sub('ä', 'a', clean_text)\n",
    "        clean_text = re.sub('ö', 'o', clean_text)\n",
    "        clean_text = re.sub('Ü', 'U', clean_text)\n",
    "        clean_text = re.sub('Ä', 'A', clean_text)\n",
    "        clean_text = re.sub('Ö', 'O', clean_text)\n",
    "        clean_text = re.sub('ß', 'sz', clean_text)\n",
    "        clean_text = re.sub('  ', ' ', clean_text)\n",
    "        clean_text = clean_text.replace(\"ECU TEST\", \"Produktname\")\n",
    "\n",
    "        # some headlines are too inconsistent to be changes by ordinary filters\n",
    "        # therefore they get cleaned here\n",
    "        clean_text = clean_text.replace(\"Job einreihen \", \"job-einreihen-tsanalysisjob\")\n",
    "        clean_text = clean_text.replace(\"Traceschrittergebnis ubernehmen \",\n",
    "                                \"traceschrittergebnis-ubernehmen-tstracestepresult\")\n",
    "        clean_text = clean_text.replace(\"Analyse anfordern \",\n",
    "                                \"analyse-anfordern-tsrequestanalysis\")\n",
    "\n",
    "        # irregular naming of a div block\n",
    "        clean_text = clean_text.replace(\"Statusleiste\", \"id1\")\n",
    "        clean_text = clean_text.replace(\"cTestBed\", \"id1\")\n",
    "        clean_text = clean_text.replace(\"IDN\", \"labcar-pincontrol-failuresim-fiu-idn\")\n",
    "\n",
    "        # remove empty div block\n",
    "        clean_text = clean_text.replace('Produktname drive', '')\n",
    "\n",
    "\n",
    "        clean_text = re.sub(' ', '-', clean_text)\n",
    "\n",
    "        \n",
    "        clean_text = clean_text.replace(\"Dokumentation-der-Analyse-im-Report\",\n",
    "                                \"dokumentation-der-analyse-jobs-im-report\")\n",
    "        clean_text = clean_text.replace(\"-Structure-With-Time\", \"structure-with-time\")\n",
    "\n",
    "        all_chap.append(clean_text)\n",
    "\n",
    "    return all_chap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13748fa4",
   "metadata": {},
   "source": [
    "#### Finde Links (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(soup_elem):\n",
    "    all_link = []\n",
    "    for link in soup_elem.find_all('a'):\n",
    "        all_link.append(link.get('href'))\n",
    "    return all_link "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d871cd2",
   "metadata": {},
   "source": [
    "#### Finde Bilder (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pic(soup_elem):\n",
    "    all_bild=[]\n",
    "    for bild in soup_elem.find_all('img'):\n",
    "        all_bild.append(bild.get('src'))\n",
    "    return all_bild"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b7855",
   "metadata": {},
   "source": [
    "### Test-Bereich für einzelnen Funktionen:\n",
    "\n",
    "In diesem Abschnitt werden die nötigen Funktionen getestet und bearbeitet, die später im oberen Teil zu einem korrekten Ergebnis zusammengeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50644572",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTMLFile = open(\"path/to/Hauptmenue.html\", \"r\", encoding = \"utf-8\")\n",
    "index = HTMLFile.read()\n",
    "S = BeautifulSoup(index,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c1b8c",
   "metadata": {},
   "source": [
    "Idee: Alle Paragrafen sind als Div Elemente durch die entsprechende Überschrift (h2) als ID versehen\n",
    "\n",
    "    - Sammle alle h2-Überschriften einer Seite\n",
    "    - Rufe anhand der Überschriften die zugehörigen h2 Divs auf\n",
    "    - extrahiere aus ihnen die p-Blöcke mit dem eigentlichen Inhalte der Seite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd970bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hauptmenu']\n"
     ]
    }
   ],
   "source": [
    "liste_h2 = get_paraName(S,\"h2\")\n",
    "print(liste_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025d608",
   "metadata": {},
   "source": [
    "Für jedes Unterkapitel (h2-Überschrift) finde alle P-Blöcke und übernimm den bereinigten Textinhalt in eine Liste. Anschließend ordne diese Liste der entsprechenden Überschrift zu.\n",
    "\n",
    "Aufbau Liste:\n",
    "\n",
    "| Paragrafen   | Inhalt                               |\n",
    "|:----------- | :---------------------------------- |\n",
    "|Überschrift 1 | (Textblock p1), (Textblock p2), .... |\n",
    "|Überschrift 2 | (Textblock p1), (textblock p2), .... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f47c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_para=[]\n",
    "ordnen = []\n",
    "for h2 in liste_h2:\n",
    "    block = S.find(\"div\",{\"id\":h2.lower()})\n",
    "    for para in block.findChildren((\"p\",\"ul\"),recursive=False):\n",
    "        clean_para.append(re.sub(\"\\n\", \" \", para.get_text().strip(), flags=re.M))\n",
    "    ordnen.append((h2, clean_para))\n",
    "print(ordnen[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3d51c",
   "metadata": {},
   "source": [
    "### Problem (gelöst durch Lösungsansatz 1)\n",
    "\n",
    "- Aktuell wird für die Überschrift h1 noch der gesamte Seiteninhalt vermerkt -> man muss also überlegen wie man hier nur den Text bis zur ersten h2-Überschrift extrahieren kann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d543ad2",
   "metadata": {},
   "source": [
    "## Lösungsansatz 1 für h1 Überschrift\n",
    "\n",
    "Da h1 ja alle anderen Überschriften mit umfasst, muss irgendwie gefiltert werden, das dieser Text nicht mit übernommen wird. \n",
    "Das Problem könnte über eine children-parent Beziehung gelöst werden:\n",
    "\n",
    "- Für die Überschrift h1 Wähle alle p-Blöcke die direkte Kinder des h1-Div-Blocks sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = S.find(\"div\",{\"id\":\"suche\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "children = block.findChildren((\"p\",\"ul\"), recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15eea2",
   "metadata": {},
   "source": [
    "Da für h1-Überschriften nur die direkten Kinder zum Auslesen interessant sind, ist es notwendig für diese eine seperate Funktion zu definieren (get_body_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "test =get_body_h1(S, get_paraName(S,\"h1\"),\"Source.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "path = 'path/to/*.html'\n",
    "files = glob.glob(path)\n",
    "for file in files:\n",
    "    name = file\n",
    "    f = open(file, 'r', encoding = \"utf-8\")\n",
    "    soup = BeautifulSoup(f.read(),'html.parser')\n",
    "    h1 = get_body_h1(soup, get_paraName(soup,\"h1\"),\"Source.html\")\n",
    "    h2 = get_body(soup, get_paraName(soup,\"h2\"),\"Source.html\")\n",
    "    all_data.append(h1)\n",
    "    all_data.append(h2)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5fb63",
   "metadata": {},
   "source": [
    "#### Evaluieren der Ausgabe und Visualisierung als DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3131a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = list(itertools.chain(*all_data))\n",
    "dataframe = pd.DataFrame(flat_list, columns = [\"Title\",\"Text\",\"Source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef26efa",
   "metadata": {},
   "source": [
    "#### Fazit zum Lösungsversuch:\n",
    "\n",
    "- Methode bringt sowohl h1 als auch h2-Überschriften und ihren Inhalt zusammen\n",
    "- Ausgelesene Daten sind weitestgehend bereinigt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dced4",
   "metadata": {},
   "source": [
    "## Fehler 2:\n",
    "Die h2-Überschriften geben aktuell alle nur den gesamten Inhalt aller Blöcke wieder anstatt nur ihren eigenen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a4432",
   "metadata": {},
   "source": [
    "### Weitere Ergänzungen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4582e",
   "metadata": {},
   "source": [
    "Es wär praktisch für die Weiterverwendung der Daten, einzelne Inhalte später eindeutig auseinander halten zu können. Hierzu würde es sich anbieten die Daten über einen Hash-Wert eindeutig auszuweisen. Da die Daten eindeutige Überschriften besitzen (?) kann dieser als Hash-Wert verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef61771",
   "metadata": {},
   "source": [
    "### Lösungsansatz 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1be910",
   "metadata": {},
   "source": [
    "- da aktuell der gesamte Inhalt pro Zwischenüberschrift übernommen wird, muss hier auf die Child-Parent Beziehung geachtet werden\n",
    "- außerdem ist es wichtig die Text-Elemente noch getrennt betrachten zu können, daher werden sie als Elemente eines Liste gespeichert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9193168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head\n",
      "Beispiel                  [Im Testfall wird auf eine Testgröße z.B. lese...\n",
      "Parametriertes-Mapping    [(Siehe auch Packagereferenzen), Das parametri...\n",
      "Globales-Mapping          [(Siehe auch Globales Mapping), Soll ein Mappi...\n",
      "Lokales-Mapping           [(Siehe auch Lokales Mapping), Für jede Testgr...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "liste = []\n",
    "for h2 in liste_h2:\n",
    "    block = S.find(\"div\",{\"id\":h2.lower()})\n",
    "    elems = (block.findChildren((\"p\",\"ul\",\"table\"),recursive =False))\n",
    "    for elem in elems:\n",
    "        text =re.sub(\"\\n\", \" \", elem.get_text().strip(), flags=re.M)\n",
    "        liste.append((text, h2))\n",
    "frame = pd.DataFrame(liste, columns = [\"Text\",\"Head\"])\n",
    "group_frame = frame.groupby('Head',sort=False)['Text'].apply(list)\n",
    "print(group_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5549c",
   "metadata": {},
   "source": [
    "### Ergänzung um Hash-Wert\n",
    "\n",
    "- Einfügen der oben getesteten Version\n",
    "- Ergänzen um einen Hash-Wert\n",
    "\n",
    "**Erfolgreich implementiert**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd724758",
   "metadata": {},
   "source": [
    "## Parsen des Text Bodies\n",
    "\n",
    "Der Textkörper wird aktuell als eine Liste von Elementen an die CSV-Datei übergeben\n",
    "Nachteil:\n",
    "    - Wir haben kein einheitliches Bild beim Auslesen der CSV\n",
    "    - Lesen wir den Text aus so kommen die Python typischen Zeichen einer Liste [] sowie Komma Trennungen mit \n",
    "        - das wollen wir nicht\n",
    "Lösung:\n",
    "    - versuchen die Daten Satzweise auszulesen und abzuspeichern\n",
    "    - damit können wir das Listen-Problem umgehen und anschließend immer noch alle Sätze einer Unterschrift und Datei zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e81970",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTMLFile = open(\"Hauptmenue.html\", \"r\", encoding = \"utf-8\")\n",
    "index = HTMLFile.read()\n",
    "S = BeautifulSoup(index,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27774d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Datei', 'Bearbeiten', 'Ansicht', 'Fenster', 'Optionen', 'Extras', 'Hilfe']\n"
     ]
    }
   ],
   "source": [
    "liste_h2 = get_paraName(S,\"h2\")\n",
    "print(liste_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e11c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = get_body(S,liste_h2,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9fb194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Type\n",
      "0  Zur Konfiguration von ECU-TEST gehören im Wese...    p\n",
      "1          Sind die Konfigurationen geladen, können:    p\n",
      "2  Die beschriebene Möglichkeit zum Starten von K...    p\n",
      "3  Die im Konfigurationswechselschritt angegebene...    p\n",
      "4  Bei der Testbenchkonfiguration (kurz TBC) werd...    p\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv ('test_cleanFiles.csv')\n",
    "print(df[[\"Text\",\"Type\"]][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af40ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (quaseldoku)",
   "language": "python",
   "name": "kedro_quaseldoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
