{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d350aea4",
   "metadata": {},
   "source": [
    "# Finetuning des Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b18c7",
   "metadata": {},
   "source": [
    "Hier soll eine Möglichkeit erarbeitet werden, mit der die HTML-Files am besten extrahiert und bereinigt werden können. Damit soll eine reibungslose Weiterverarbeitung der Texte ermöglicht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1ec6a",
   "metadata": {},
   "source": [
    "## Vorüberlegung:\n",
    "\n",
    "Für unser Projekt sind verschiedene Teile des HTML-Files besonders von Bedeutung:\n",
    "\n",
    "- Text: Inhalt der HTML-Seite\n",
    "    - Diese sollte wenn möglich in einzelne Abschnitte gegliedert werden (ermöglicht später die exakte Beantwortung von Fragen)\n",
    "    - Unwichtige Elemente der Seite (wie Menü-Punkte, die auf jeder Seite erscheinen) gilt es zu filtern\n",
    "    - Nichtdarstellbare Zeichen gilt es zu filtern\n",
    "- Überschrift und Unterüberschrift der Paragraphen\n",
    "    - Wenn man im späteren Verlauf auf eine Frage einen Paragrafen referenzieren möchte, ist es wichtig die zum Paragraph gehörende Unterüberschrift und die Verlinkung dahin zu vermerken\n",
    "- HTML-File: Der Link zum verwendeten File sollte als Meta-Data vermerkt werden um später zur Seite navigieren zu können\n",
    "    - (vielleicht kann man noch den Link zur genauen Teil-Überschrift extrahieren)\n",
    "- (OPTIONAL Links und Bilder: Man könnte die verwendeten Links (href) und den Pfad zu den Bildern mit extrahieren, stellt sich aber die Frage, in wie weit das relevant ist) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa810f3",
   "metadata": {},
   "source": [
    "### Benötigte Pakete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d131563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6ea83",
   "metadata": {},
   "source": [
    "(Ich habe Probleme in kedro jupyter notebooks das Paket pandas zu verwenden, mit diesem workaround ist es mir aber möglich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24517f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2be45",
   "metadata": {},
   "source": [
    "### Auslesen von HTML-Elementen\n",
    "\n",
    "Hier wird zunächst nur repräsentativ ein kleiner Teil der Dokumentationsdateien ausgelesen (alle Seiten zum Punkt \"Bedienung\").\n",
    "Die ausgelesenen Daten werden in einem CSV-File gespeichert:\n",
    "\n",
    "|Title| Text| Source |\n",
    "|:----|:----|:-------|\n",
    "| h1  |...  | file1  |\n",
    "| h2  |...  | file1  |\n",
    "| h1  |...  | file2  |\n",
    "| h2  |...  | file2  |\n",
    "| h2  |...  | file2  |\n",
    "| ... |...  | ...    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "db1eae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'path/to/*.html'\n",
    "files = glob.glob(path)\n",
    "all_data = []\n",
    "for file in files:\n",
    "    f = open(file, 'r', encoding = \"utf-8\")\n",
    "    name = file\n",
    "    soup = BeautifulSoup(f.read(),'html.parser')\n",
    "    h1 = get_body(soup, get_paraName(soup,\"h1\"),name)\n",
    "    h2 = get_body(soup, get_paraName(soup,\"h2\"),name)\n",
    "    all_data.append(h1)\n",
    "    all_data.append(h2)\n",
    "    f.close()\n",
    "    #write the data\n",
    "flat_list = list(itertools.chain(*all_data))\n",
    "dataframe = pd.DataFrame(flat_list, columns = [\"Title\",\"Source\",\"Hash\",\"Text\"])\n",
    "dataframe.to_csv(\"test_cleanFiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b6dd0",
   "metadata": {},
   "source": [
    "#### Erzeugen des Text-Body Elements\n",
    "\n",
    "Im Div-Block mit der Klasse \"document\" wird der Inhalt der Seite umschlossen (ausgeschlossen das Seiten Menü, Header und Fußzeile)\n",
    "\n",
    "Die Unterteilungen in der Seite erfolgen durch kleinere Div-Blöcke der Klasse \"section\".\n",
    "Die id der Div-Blöcke entspricht der Überschrift (h1 oder h2).\n",
    "Daher werden nun Anhand der Überschriften zunächst die Div-Blöcke gesucht und anschließend die p-Blöcke, Auflistungen (ul) und Tabellen (table) ausgelesen. Diese werden als einzelne Elemente einer Liste abgespeichert.\n",
    "\n",
    "Zudem wird noch ein eindeutiger Hashwert auf Basis der aktuellen Überschrift erstell, um später die Elemente eindeutig ausweisen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a4f514b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(soup_elem,liste_headlines,name):\n",
    "    ordnen = []\n",
    "    for h in liste_headlines:\n",
    "        block = soup_elem.find(\"div\",{\"id\":h.lower()})\n",
    "        elems = (block.findChildren((\"p\",\"ul\",\"table\"),recursive =False))\n",
    "        for elem in elems:\n",
    "            text = re.sub(\" \\n \", \" \", elem.get_text().strip(), flags=re.M)\n",
    "            text = re.sub(\"\\n \", \" \", text.strip(), flags=re.M)\n",
    "            text = re.sub(\" \\n\", \" \", text.strip(), flags=re.M)\n",
    "            text =re.sub(\"\\n\", \" \", text.strip(), flags=re.M)\n",
    "            ordnen.append((text, h, name, hash(h)))\n",
    "    text_frame = pd.DataFrame(ordnen, columns = [\"Text\",\"Head\",\"Link\",\"Hash\"])\n",
    "    text_merge = text_frame.groupby(['Head',\"Link\",\"Hash\"],sort=False)['Text'].apply(list)\n",
    "    \n",
    "    final = text_merge.reset_index().values.tolist()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b18cc",
   "metadata": {},
   "source": [
    "#### Erzeugen des Text-Body Elements (VERALTET)\n",
    "\n",
    "Aktuell wird der Teil für h1-Überschriften getrennt betrachtet, da hier spezielle Parent-children Bezeihungen verwendet werden\n",
    "um den Text der nur zur h1-Überschrift gehört zu extrahieren.\n",
    "\n",
    "Da die Funktion get_body() zur korrekten Teilung der h2 Überschriften angepasst werden musste, ist diese Funktion nun hinfällig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body_h1(soup_elem,liste_headlines ,name):\n",
    "    clean_para=[]\n",
    "    ordnen = []\n",
    "    for h1 in liste_headlines:\n",
    "        block = soup_elem.find(\"div\",{\"id\":h1.lower()})\n",
    "        for para in block.findChildren((\"p\",\"ul\"), recursive=False):\n",
    "            clean_para.append(re.sub(\"\\n\", \" \", para.get_text().strip(), flags=re.M))\n",
    "        ordnen.append((h1, clean_para,name))\n",
    "    return ordnen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61018b8a",
   "metadata": {},
   "source": [
    "#### Finde Kapitel\n",
    "\n",
    "Es müssen die Kapitelnamen und die Namen der Unterkapitel für jede Datei gesucht werden. Nur so kann später der Inhalt den Überschriften zugeordnet werden.\n",
    "Ein zu der Überschrift gehörender Block, trägt meist den Namen der dazugehördenen Überschrift. Ausnahmen entstehen durch Sonderzeichen.\n",
    "Da die h1-Überschriften anders zu behandeln sind als h2-Überschriften muss als Parameter übergeben werden welcher Typ von Überschrift (headline \"h1\" oder \"h2\") gerade betrachtet wird.\n",
    "\n",
    "Überlegung: Zur Zeit werden die Namen der Überschriften noch umständlich angepasst, um als id der zugehörigen Div-Blöcke verwendet zu werden. Vielleicht gibt es da einen besseren Work-Around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5525a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paraName(soup_elem, headline):\n",
    "    all_unter = []\n",
    "    for unterkap in soup_elem.find_all(headline):\n",
    "        sauber = re.sub('\\uf0c1','', unterkap.get_text())\n",
    "        string = sauber.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "        sauber = re.sub('\\W+',' ',string)\n",
    "        sauber = re.sub('ü','u',sauber)\n",
    "        sauber = re.sub('ä','a',sauber)\n",
    "        sauber = re.sub('ö','o',sauber)\n",
    "        sauber = re.sub('Ü','U',sauber)\n",
    "        sauber = re.sub('Ä','A',sauber)\n",
    "        sauber = re.sub('Ö','O',sauber)\n",
    "        sauber = sauber.replace(\"ECU TEST\",\"Produktname\")\n",
    "        all_unter.append(re.sub(' ','-', sauber))\n",
    "    return all_unter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13748fa4",
   "metadata": {},
   "source": [
    "#### Finde Links (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(soup_elem):\n",
    "    all_link = []\n",
    "    for link in soup_elem.find_all('a'):\n",
    "        all_link.append(link.get('href'))\n",
    "    return all_link "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d871cd2",
   "metadata": {},
   "source": [
    "#### Finde Bilder (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pic(soup_elem):\n",
    "    all_bild=[]\n",
    "    for bild in soup_elem.find_all('img'):\n",
    "        all_bild.append(bild.get('src'))\n",
    "    return all_bild"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b7855",
   "metadata": {},
   "source": [
    "### Test-Bereich für einzelnen Funktionen:\n",
    "\n",
    "In diesem Abschnitt werden die nötigen Funktionen getestet und bearbeitet, die später im oberen Teil zu einem korrekten Ergebnis zusammengeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50644572",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTMLFile = open(\"path/to/file.html\", \"r\", encoding = \"utf-8\")\n",
    "index = HTMLFile.read()\n",
    "S = BeautifulSoup(index,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c1b8c",
   "metadata": {},
   "source": [
    "Idee: Alle Paragrafen sind als Div Elemente durch die entsprechende Überschrift (h2) als ID versehen\n",
    "\n",
    "    - Sammle alle h2-Überschriften einer Seite\n",
    "    - Rufe anhand der Überschriften die zugehörigen h2 Divs auf\n",
    "    - extrahiere aus ihnen die p-Blöcke mit dem eigentlichen Inhalte der Seite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd970bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_h2 = get_paraName(S,\"h2\")\n",
    "print(liste_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025d608",
   "metadata": {},
   "source": [
    "Für jedes Unterkapitel (h2-Überschrift) finde alle P-Blöcke und übernimm den bereinigten Textinhalt in eine Liste. Anschließend ordne diese Liste der entsprechenden Überschrift zu.\n",
    "\n",
    "Aufbau Liste:\n",
    "\n",
    "| Paragrafen   | Inhalt                               |\n",
    "|:----------- | :---------------------------------- |\n",
    "|Überschrift 1 | (Textblock p1), (Textblock p2), .... |\n",
    "|Überschrift 2 | (Textblock p1), (textblock p2), .... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f47c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_para=[]\n",
    "ordnen = []\n",
    "for h2 in liste_h2:\n",
    "    block = S.find(\"div\",{\"id\":h2.lower()})\n",
    "    for para in block.findChildren((\"p\",\"ul\"),recursive=False):\n",
    "        clean_para.append(re.sub(\"\\n\", \" \", para.get_text().strip(), flags=re.M))\n",
    "    ordnen.append((h2, clean_para))\n",
    "print(ordnen[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3d51c",
   "metadata": {},
   "source": [
    "### Problem (gelöst durch Lösungsansatz 1)\n",
    "\n",
    "- Aktuell wird für die Überschrift h1 noch der gesamte Seiteninhalt vermerkt -> man muss also überlegen wie man hier nur den Text bis zur ersten h2-Überschrift extrahieren kann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d543ad2",
   "metadata": {},
   "source": [
    "## Lösungsansatz 1 für h1 Überschrift\n",
    "\n",
    "Da h1 ja alle anderen Überschriften mit umfasst, muss irgendwie gefiltert werden, das dieser Text nicht mit übernommen wird. \n",
    "Das Problem könnte über eine children-parent Beziehung gelöst werden:\n",
    "\n",
    "- Für die Überschrift h1 Wähle alle p-Blöcke die direkte Kinder des h1-Div-Blocks sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = S.find(\"div\",{\"id\":\"suche\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "children = block.findChildren((\"p\",\"ul\"), recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15eea2",
   "metadata": {},
   "source": [
    "Da für h1-Überschriften nur die direkten Kinder zum Auslesen interessant sind, ist es notwendig für diese eine seperate Funktion zu definieren (get_body_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "test =get_body_h1(S, get_paraName(S,\"h1\"),\"Source.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "path = 'path/to/*.html'\n",
    "files = glob.glob(path)\n",
    "for file in files:\n",
    "    name = file\n",
    "    f = open(file, 'r', encoding = \"utf-8\")\n",
    "    soup = BeautifulSoup(f.read(),'html.parser')\n",
    "    h1 = get_body_h1(soup, get_paraName(soup,\"h1\"),\"Source.html\")\n",
    "    h2 = get_body(soup, get_paraName(soup,\"h2\"),\"Source.html\")\n",
    "    all_data.append(h1)\n",
    "    all_data.append(h2)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5fb63",
   "metadata": {},
   "source": [
    "#### Evaluieren der Ausgabe und Visualisierung als DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3131a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = list(itertools.chain(*all_data))\n",
    "dataframe = pd.DataFrame(flat_list, columns = [\"Title\",\"Text\",\"Source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef26efa",
   "metadata": {},
   "source": [
    "#### Fazit zum Lösungsversuch:\n",
    "\n",
    "- Methode bringt sowohl h1 als auch h2-Überschriften und ihren Inhalt zusammen\n",
    "- Ausgelesene Daten sind weitestgehend bereinigt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dced4",
   "metadata": {},
   "source": [
    "## Fehler 2:\n",
    "Die h2-Überschriften geben aktuell alle nur den gesamten Inhalt aller Blöcke wieder anstatt nur ihren eigenen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a4432",
   "metadata": {},
   "source": [
    "### Weitere Ergänzungen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4582e",
   "metadata": {},
   "source": [
    "Es wär praktisch für die Weiterverwendung der Daten, einzelne Inhalte später eindeutig auseinander halten zu können. Hierzu würde es sich anbieten die Daten über einen Hash-Wert eindeutig auszuweisen. Da die Daten eindeutige Überschriften besitzen (?) kann dieser als Hash-Wert verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef61771",
   "metadata": {},
   "source": [
    "### Lösungsansatz 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1be910",
   "metadata": {},
   "source": [
    "- da aktuell der gesamte Inhalt pro Zwischenüberschrift übernommen wird, muss hier auf die Child-Parent Beziehung geachtet werden\n",
    "- außerdem ist es wichtig die Text-Elemente noch getrennt betrachten zu können, daher werden sie als Elemente eines Liste gespeichert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9193168",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = []\n",
    "for h2 in liste_h2:\n",
    "    block = S.find(\"div\",{\"id\":h2.lower()})\n",
    "    elems = (block.findChildren((\"p\",\"ul\",\"table\"),recursive =False))\n",
    "    for elem in elems:\n",
    "        text =re.sub(\"\\n\", \" \", elem.get_text().strip(), flags=re.M)\n",
    "        liste.append((text, h2))\n",
    "frame = pd.DataFrame(liste, columns = [\"Text\",\"Head\"])\n",
    "group_frame = frame.groupby('Head',sort=False)['Text'].apply(list)\n",
    "print(group_frame[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5549c",
   "metadata": {},
   "source": [
    "### Ergänzung um Hash-Wert\n",
    "\n",
    "- Einfügen der oben getesteten Version\n",
    "- Ergänzen um einen Hash-Wert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b6e81970",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_body(S, liste_h2, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "27774d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = pd.DataFrame(test,columns=[\"Head\",\"Link\",\"Hash\",\"Body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c5e11c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Head  Link                 Hash  \\\n",
      "0             Terminologie  test  2232245079872126677   \n",
      "1            Konfiguration  test  -226203862521301321   \n",
      "2  Arbeiten-mit-Attributen  test  8594958551157606788   \n",
      "3    Arbeiten-mit-Packages  test  3299522519948488173   \n",
      "4   Arbeiten-mit-Projekten  test -4508554425207993390   \n",
      "\n",
      "                                                Body  \n",
      "0  [ECU-TEST und die jeweiligen Testmanagement-Sy...  \n",
      "1  [Vor der ersten Verwendung muss die Schnittste...  \n",
      "2  [Packages und Projekte können in ECU-TEST mit ...  \n",
      "3  [Mit ECU-TEST kann man Packages als Testskript...  \n",
      "4  [ECU-TEST kann Projekte als Testsuiten in das ...  \n"
     ]
    }
   ],
   "source": [
    "print(test_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c666c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (quaseldoku)",
   "language": "python",
   "name": "kedro_quaseldoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
