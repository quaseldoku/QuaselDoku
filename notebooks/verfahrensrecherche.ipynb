{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e901fd35",
   "metadata": {},
   "source": [
    "# ü§ó Verfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb482fba",
   "metadata": {},
   "source": [
    "## Welche Verfahren sind geeignet?\n",
    "\n",
    "Alle QA-Modelle basieren auf vortrainierten Sprachmodellen, die allermeisten auf Transformer-Architekturen (v.a. BERT). Die Basismodelle sind auf allgemeinen Task trainiert (zum Beispiel Masking, Next Token Prediction). Der Vorteil ist, dass dieses Training auf riesigen nicht annotierten Datenmengen erfolgen kann. Dieses Verfahren kann man als unsupervised bezeichnen, da die Daten nicht annotiert sind, bzw. die n√∂tige Annotation (maskieren von Tokens) automatisiert erfolgen kann.\n",
    "\n",
    "Ein Finetuning auf einem Domain spezifischen Datensatz (als Vorstufe zum Finetuning auf einen Domain spezifischen Task, wie QA) ist denkbar aber anscheinend nicht die Regel [1].\n",
    "\n",
    "Grunds√§tzlich suchen wir nach Modellen f√ºr Extractive QA, wobei das Modell eine Antwort aus einen gegebenen Kontext extrahiert (verglichen etwa mit Open Generative QA, wobei eine Freitext-Antwort generiert wird) [2]. \n",
    "\n",
    "Die Allermeisten Extractive-QA-Modelle sind so trainiert, dass die Prediction des Modells bei einer gegeben Frage + Kontext zwei Wahrscheinlichkeiten ausgibt: Eine f√ºr den Start der Antwort Sequenz und eine f√ºr das Ende. Die Textpassage zwischen den beiden Tokens mit der h√∂chsten Wahrscheinlichkeit ist dann die Antwort. Im Falle, dass der End Token vor dem Anfangs Token auftritt, ist die Antwort ein leerer String [3].\n",
    "\n",
    "[1] https://discuss.huggingface.co/t/fine-tuning-bert-model-on-domain-specific-language-and-for-classification/3106/6 \\\n",
    "[2] https://huggingface.co/tasks/question-answering \\\n",
    "[3] https://blog.paperspace.com/how-to-train-question-answering-machine-learning-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468b3c",
   "metadata": {},
   "source": [
    "## Vortrainiertes Netz ausreichend / Dom√§nenspezifisches Training notwendig?\n",
    "\n",
    "Das Basis NLP-Modell wird (aufgabenspezifisch) ge-finetuned an Hand eines - f√ºr den Ziel Task annotierten - Datensatzes. F√ºr QA-Modelle ist das meistens SQuAD oder SQuAD 2 [1]. Es gibt eine Vielzahl an NLP-Modellen, die auf diesen Task an Hand einer der beiden Datens√§tze ge-finetuned sind, da dieser jedoch in Englisch ist, sind diese f√ºr deutsches QA nicht geeignet. \n",
    "\n",
    "Es gibt jedoch auch einen GermanQaAD [2] und dazugeh√∂rige finetuned Modelle [3]. Manchen von diesen sind allerdings auch auf deQuAD trainiert, einer (automatischen) √úbersetzung von SuAD ins Deutsche. \n",
    "\n",
    "F√ºr die meisten QA-Task reichen die vortrainierten Modelle aus (die auf eher generischen Daten trainiert wurden, wie Wikipedia-Artikel zu verschiedensten Themen). F√ºr stark spezifische Domains empfiehlt sich jedoch entweder ein bereits auf SQuAD (oder GermanQaAD) getuntes Modell noch weiter auf einem annotierten, domainspezifischen Datensatz zu tunen, oder (wenn der eigene Datensatz gro√ü genug ist) ein Basismodell (wie BERT) auf den spezifischen QA-Task zu tunen. Als Gr√∂√üenordnung sind hier ~2000 ben√∂tigte Samples zu finden [4]. Ein How-To ist hier [5] zu finden. \n",
    "\n",
    "Wenn man ausgehend von einem NLP-Basismodell Task spezifisch finetuned, dann ist die Definition des Task offen, das hei√üt zum Beispiel ein reines Zuordnen einer Frage zu einem Dokument in der Knowledge Base w√§re m√∂glich (verglichen mit dem - durch vortrainierten Modelle gegeben - Task der Ausgabe der genauen Anfangs- und End Tokens der Antwort Passage). \n",
    "\n",
    "Das Finetuning ist (im Vergleich zum Training des Basismodells) also supervised. Ein unsupervised Finetuning (Finetuning eines QA-Netzes ohne annotierte Daten) ist nicht m√∂glich.\n",
    "\n",
    "![M√∂gliche Workflows f√ºr die Nutzung / Training von QA-Modellen](model_workflows.png)\n",
    "*M√∂gliche Workflows f√ºr die Nutzung / Training von QA-Modellen* \n",
    "\n",
    "\n",
    "[1] https://rajpurkar.github.io/SQuAD-explorer/ \\\n",
    "[2] https://arxiv.org/pdf/2104.12741.pdf \\\n",
    "[3] https://huggingface.co/models?language=de&pipeline_tag=question-answering&sort=downloads \\\n",
    "[4] (https://haystack.deepset.ai/docs/tutorial2md \\\n",
    "[5] https://huggingface.co/transformers/v3.2.0/custom_datasets.html#question-answering-with-squad-2-0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039dcf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "458f1c18",
   "metadata": {},
   "source": [
    "# üíæ Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab80e5",
   "metadata": {},
   "source": [
    "## Ground Truth / Wie m√ºssen die Daten gelabelt sein?\n",
    "\n",
    "Als Ground Truth bietet sich ein Datensatz an, der eine Reihe von Fragen mit den dazugeh√∂rigen Text Passagen aus Doku beinhaltet (dem Format von SQuAD folgend). Dabei werden mehrere Fragen pro Kontext formuliert, und mehrere Antworten auf jede Frage annotiert [1] (Das gilt f√ºr das Validation Dataset, bei dem auch Fragen ohne Antworten vorkommen k√∂nnen (negative samples - Verh√§ltnis ~50/50); f√ºr das Trainings Set gibt es jedoch nur positive samples und eine Antwort pro Frage [2]).\n",
    "\n",
    "Gegen√ºber diesem k√∂nnen die Verfahren verglichen werden. \n",
    "\n",
    "Dieser Datensatz muss h√§ndisch annotiert werden. Daf√ºr w√ºrde sich das Annotation Tool von Haystack anbieten [3]. Der Umfang h√§ngt davon ab, ob der Datensatz als reiner Testdatensatz oder zum Finetuning des Modells verwendet werden soll. Als Annotations Referenz kann der von Haystack ver√∂ffentlichte Leitfaden verwendet werden [4].\n",
    "\n",
    "Bez√ºglich des Umfangs lassen sich keine genauen Angaben finden, letztendlich soll er die Breite aller m√∂glichen Fragen abfassen [5].\n",
    "\n",
    "[1] https://huggingface.co/datasets/squad/viewer/plain_text/validation \\\n",
    "[2] https://huggingface.co/datasets/squad \\\n",
    "[3] https://github.com/deepset-ai/haystack/tree/master/annotation_tool \\\n",
    "[4] https://haystack.deepset.ai/guides/annotation \\\n",
    "[5] https://qa.fastforwardlabs.com/domain%20adaptation/transfer%20learning/specialized%20datasets/qa/medical%20qa/2020/07/22/QA-for-Specialized-Data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f4b34",
   "metadata": {},
   "source": [
    "## Welche Daten sind notwendig (pro Verfahren) / Welche Features brauchen wir?\n",
    "\n",
    "F√ºr alle Verfahren reicht der Textk√∂rper aus. Je nach Verfahren sind weitere Preprocessing Schritte notwendig, wie etwa padding eines Satzes auf eine fixe L√§nge, wof√ºr sowohl PyTorch [?] als auch die Huggingface API [?] Funktionen zur Verf√ºgung stellen.\n",
    "\n",
    "F√ºr die Anwendung selbst m√ºssen im Preprocessing jedoch noch weitere Informationen gespeichert werden, wie etwa \n",
    "\n",
    "Pfad des html-Dokuments\n",
    "Link zum Paragraf im html-Dokument\n",
    "\n",
    "um die gefundene Antwort in der Dokumentation ausfindig machen zu k√∂nnen. \n",
    "\n",
    "Das h√§ngt schlie√ülich davon ab, was als Dokument begriffen wird\n",
    "\n",
    "- html-Datei (1 Seite der Doku)\n",
    "- Unterabschnitt einer Seite (div / p / ul / li)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b709e4",
   "metadata": {},
   "source": [
    "## Wie werden die Features repr√§sentiert?\n",
    "\n",
    "Features k√∂nnten als Spalten einer Tabelle (PandasDataFrame / csv) repr√§sentiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c6714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87278b97",
   "metadata": {},
   "source": [
    "# üßÆ Vergleich von Modellen / Baseline Modelle / Metriken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c13524",
   "metadata": {},
   "source": [
    "## M√∂gliche Baseline Modelle\n",
    "\n",
    "### Stichwortsuche\n",
    "\n",
    "### Bag of Words (?)\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "[1] https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630\n",
    "\n",
    "### Sentence Similarity\n",
    "\n",
    "[1] https://huggingface.co/tasks/sentence-similarity\n",
    "\n",
    "### Passage Ranking\n",
    "\n",
    "Passage Ranking Modelle nehmen ein Set an Dokumenten und eine Anfrage Query und geben Ranking der Dokumente heraus, wie gut die Query auf alle verf√ºgbaren Dokumente passt [1]. Das k√∂nnte man als simples QA Modell verwenden, indem die Query die Frage und die Dokumente die Knowledge Base darstellen. \n",
    "\n",
    "Passage Ranking bietet sich auch Vorverarbeitungsschritt einer Anfrage an ein QA-Modell an. So kann Inferenzzeit gespart werden, wenn verf√ºgbare Dokumente in der Knowledge Base erst nach ihrer Relevanz sortiert und dann die Antwort nur im relevantesten (bzw in den n meist relevanten) gesucht wird [2].\n",
    "\n",
    "[1] https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco \\\n",
    "[2] https://huggingface.co/tasks/question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22587b",
   "metadata": {},
   "source": [
    "## quantitative G√ºtema√üe\n",
    "\n",
    "### Exact Match (EM)\n",
    "\n",
    "F√ºr jedes Frage / Antwort Paar wird die Prediction gegen√ºber der Ground Truth verglichen. Nur bei einer genauen √úberlappung der Text Passage wird die Prediction als ‚ÄúTreffer‚Äù gez√§hlt. Verschiebung um eine Position im Satz, macht die Prediction zu einem Miss.\n",
    "\n",
    "### F1-Score\n",
    "Hier z√§hlt die Schnittmenge von Prediction und wahrer Antwort. Precision ist dabei die Anzahl der W√∂rter in der Schnittmenge gegen√ºber aller W√∂rter in der Prediction, Recall ist das Verh√§ltnis der W√∂rter in der Schnittmenge zu allen W√∂rtern in der Ground Truth. \n",
    "\n",
    "![equation f1 score](f1_score.png)\n",
    "\n",
    "### Eigene\n",
    "\n",
    "Je nach Task k√∂nnen eigene Metriken definiert werden: Ist die Aufgabe etwa eine Frage nur einem Abschnitt zuzuordnen, ist die genaue Positionierung der Satztokens jedoch irrelevant, k√∂nnte eine Accuracy angegeben werden, in wie vielen F√§llen die Zuordnung zum Richtigen Dokument erfolgte. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa42f0",
   "metadata": {},
   "source": [
    "## Public Benchmarks /  Welche gibt es Speziell im NLP-Bereich?\n",
    "\n",
    "QA-Modelle werden auf einer Reihe von Datasets gebenchmarkt, vorne dran SQuAD 1 & SQuAD 2, sowie die jeweiligen dev Splits (Validierungsdaten mit negativen Samples und Mehrfachantworten) [1].\n",
    "\n",
    "[2] zeigt etwa eine √úbersicht der Performance der besten Modelle auf dem SQuAD 2 Datensatz.  \n",
    "\n",
    "Die Relevanz f√ºr unser Problem ist aber ein bisschen fraglich, denn die Modelle m√ºssen zum einen vortrainiert verf√ºgbar sein, zum anderen auf deutschen Datens√§tzen finetuned sein. \n",
    "\n",
    "[1] https://paperswithcode.com/task/question-answering \\\n",
    "[2] https://paperswithcode.com/sota/question-answering-on-squad20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff3d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb64c3cf",
   "metadata": {},
   "source": [
    "# ‚ùìFragen Backlog\n",
    "\n",
    "### Qualitativer Verfahrensvergleich auf Problemstellung bezogen\n",
    "\n",
    "### Quantitativer, grunds√§tzlicher Verfahrensvergleich mit Benchmarks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (quaseldoku)",
   "language": "python",
   "name": "kedro_quaseldoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
