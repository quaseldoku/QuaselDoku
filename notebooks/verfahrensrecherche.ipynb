{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346851a8",
   "metadata": {},
   "source": [
    "# ü§ó Verfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d837bf2",
   "metadata": {},
   "source": [
    "## Welche Verfahren sind geeignet?\n",
    "\n",
    "Alle QA-Modelle basieren auf vortrainierten Sprachmodellen, die allermeisten auf Transformer-Architekturen (v.a. BERT). Die Basismodelle sind auf allgemeinen Task trainiert (zum Beispiel Masking, Next Token Prediction). Der Vorteil ist, dass dieses Training auf riesigen nicht annotierten Datenmengen erfolgen kann. Dieses Verfahren kann man als unsupervised bezeichnen, da die Daten nicht annotiert sind, bzw. die n√∂tige Annotation (maskieren von Tokens) automatisiert erfolgen kann.\n",
    "\n",
    "Ein Finetuning auf einem Domain spezifischen Datensatz (als Vorstufe zum Finetuning auf einen Domain spezifischen Task, wie QA) ist denkbar aber anscheinend nicht die Regel [1].\n",
    "\n",
    "Grunds√§tzlich suchen wir nach Modellen f√ºr Extractive QA, wobei das Modell eine Antwort aus einen gegebenen Kontext extrahiert (verglichen etwa mit Open Generative QA, wobei eine Freitext-Antwort generiert wird) [2]. \n",
    "\n",
    "Die Allermeisten Extractive-QA-Modelle sind so trainiert, dass die Prediction des Modells bei einer gegeben Frage + Kontext zwei Wahrscheinlichkeiten ausgibt: Eine f√ºr den Start der Antwort Sequenz und eine f√ºr das Ende. Die Textpassage zwischen den beiden Tokens mit der h√∂chsten Wahrscheinlichkeit ist dann die Antwort. Im Falle, dass der End Token vor dem Anfangs Token auftritt, ist die Antwort ein leerer String [3].\n",
    "\n",
    "[1] https://discuss.huggingface.co/t/fine-tuning-bert-model-on-domain-specific-language-and-for-classification/3106/6 <br>\n",
    "[2] https://huggingface.co/tasks/question-answering <br>\n",
    "[3] https://blog.paperspace.com/how-to-train-question-answering-machine-learning-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2f0ac",
   "metadata": {},
   "source": [
    "## Vortrainiertes Netz ausreichend / Dom√§nenspezifisches Training notwendig?\n",
    "\n",
    "Das Basis NLP-Modell wird (aufgabenspezifisch) ge-finetuned an Hand eines - f√ºr den Ziel Task annotierten - Datensatzes. F√ºr QA-Modelle ist das meistens SQuAD oder SQuAD 2 [1]. Es gibt eine Vielzahl an NLP-Modellen, die auf diesen Task an Hand einer der beiden Datens√§tze ge-finetuned sind, da dieser jedoch in Englisch ist, sind diese f√ºr deutsches QA nicht geeignet. \n",
    "\n",
    "Es gibt jedoch auch einen GermanQaAD [2] und dazugeh√∂rige finetuned Modelle [3]. Manchen von diesen sind allerdings auch auf deQuAD trainiert, einer (automatischen) √úbersetzung von SuAD ins Deutsche. \n",
    "\n",
    "F√ºr die meisten QA-Task reichen die vortrainierten Modelle aus (die auf eher generischen Daten trainiert wurden, wie Wikipedia-Artikel zu verschiedensten Themen). F√ºr stark spezifische Domains empfiehlt sich jedoch entweder ein bereits auf SQuAD (oder GermanQaAD) getuntes Modell noch weiter auf einem annotierten, domainspezifischen Datensatz zu tunen, oder (wenn der eigene Datensatz gro√ü genug ist) ein Basismodell (wie BERT) auf den spezifischen QA-Task zu tunen. Als Gr√∂√üenordnung sind hier ~2000 ben√∂tigte Samples zu finden [4]. Ein How-To ist hier [5] zu finden. \n",
    "\n",
    "Wenn man ausgehend von einem NLP-Basismodell Task spezifisch finetuned, dann ist die Definition des Task offen, das hei√üt zum Beispiel ein reines Zuordnen einer Frage zu einem Dokument in der Knowledge Base w√§re m√∂glich (verglichen mit dem - durch vortrainierten Modelle gegeben - Task der Ausgabe der genauen Anfangs- und End Tokens der Antwort Passage). \n",
    "\n",
    "Das Finetuning ist (im Vergleich zum Training des Basismodells) also supervised. Ein unsupervised Finetuning (Finetuning eines QA-Netzes ohne annotierte Daten) ist nicht m√∂glich.\n",
    "\n",
    "![M√∂gliche Workflows f√ºr die Nutzung / Training von QA-Modellen](./imgs/model_workflows.png)\n",
    "*M√∂gliche Workflows f√ºr die Nutzung / Training von QA-Modellen* \n",
    "\n",
    "\n",
    "[1] https://rajpurkar.github.io/SQuAD-explorer/ <br>\n",
    "[2] https://arxiv.org/pdf/2104.12741.pdf <br>\n",
    "[3] https://huggingface.co/models?language=de&pipeline_tag=question-answering&sort=downloads <br>\n",
    "[4] (https://haystack.deepset.ai/docs/tutorial2md <br>\n",
    "[5] https://huggingface.co/transformers/v3.2.0/custom_datasets.html#question-answering-with-squad-2-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e734e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e59eac03",
   "metadata": {},
   "source": [
    "# üíæ Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722e3bf",
   "metadata": {},
   "source": [
    "## Ground Truth / Wie m√ºssen die Daten gelabelt sein?\n",
    "\n",
    "Als Ground Truth bietet sich ein Datensatz an, der eine Reihe von Fragen mit den dazugeh√∂rigen Text Passagen aus Doku beinhaltet (dem Format von SQuAD folgend). Dabei werden mehrere Fragen pro Kontext formuliert, und mehrere Antworten auf jede Frage annotiert [1] (Das gilt f√ºr das Validation Dataset, bei dem auch Fragen ohne Antworten vorkommen k√∂nnen (negative samples - Verh√§ltnis ~50/50); f√ºr das Trainings Set gibt es jedoch nur positive samples und eine Antwort pro Frage [2]).\n",
    "\n",
    "Gegen√ºber diesem k√∂nnen die Verfahren verglichen werden. \n",
    "\n",
    "Dieser Datensatz muss h√§ndisch annotiert werden. Daf√ºr w√ºrde sich das Annotation Tool von Haystack anbieten [3]. Der Umfang h√§ngt davon ab, ob der Datensatz als reiner Testdatensatz oder zum Finetuning des Modells verwendet werden soll. Als Annotations Referenz kann der von Haystack ver√∂ffentlichte Leitfaden verwendet werden [4].\n",
    "\n",
    "Bez√ºglich des Umfangs lassen sich keine genauen Angaben finden, letztendlich soll er die Breite aller m√∂glichen Fragen abfassen [5].\n",
    "\n",
    "[1] https://huggingface.co/datasets/squad/viewer/plain_text/validation <br>\n",
    "[2] https://huggingface.co/datasets/squad <br>\n",
    "[3] https://github.com/deepset-ai/haystack/tree/master/annotation_tool <br>\n",
    "[4] https://haystack.deepset.ai/guides/annotation <br>\n",
    "[5] https://qa.fastforwardlabs.com/domain%20adaptation/transfer%20learning/specialized%20datasets/qa/medical%20qa/2020/07/22/QA-for-Specialized-Data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db33dc",
   "metadata": {},
   "source": [
    "## Welche Daten sind notwendig (pro Verfahren) / Welche Features brauchen wir?\n",
    "\n",
    "F√ºr alle Verfahren reicht der Textk√∂rper aus. Je nach Verfahren sind weitere Preprocessing Schritte notwendig, wie etwa padding eines Satzes auf eine fixe L√§nge, wof√ºr sowohl PyTorch [?] als auch die Huggingface API [?] Funktionen zur Verf√ºgung stellen.\n",
    "\n",
    "F√ºr die Anwendung selbst m√ºssen im Preprocessing jedoch noch weitere Informationen gespeichert werden, wie etwa \n",
    "\n",
    "Pfad des html-Dokuments\n",
    "Link zum Paragraf im html-Dokument\n",
    "\n",
    "um die gefundene Antwort in der Dokumentation ausfindig machen zu k√∂nnen. \n",
    "\n",
    "Das h√§ngt schlie√ülich davon ab, was als Dokument begriffen wird\n",
    "\n",
    "- html-Datei (1 Seite der Doku)\n",
    "- Unterabschnitt einer Seite (div / p / ul / li)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa588b7",
   "metadata": {},
   "source": [
    "## Wie werden die Features repr√§sentiert?\n",
    "\n",
    "Features k√∂nnten als Spalten einer Tabelle (PandasDataFrame / csv) repr√§sentiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5cc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c50015f",
   "metadata": {},
   "source": [
    "# üßÆ Vergleich von Modellen / Baseline Modelle / Metriken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6037ffb",
   "metadata": {},
   "source": [
    "## M√∂gliche Baseline Modelle\n",
    "\n",
    "### Stichwortsuche\n",
    "Am naheliegensten ist es, die Stichwortsuche als Baseline zu benutzen. Hierbei k√∂nnen jedoch nur Vergleiche aus Dokumenten-Ebene gezogen werden, also ob bei einer gegebenen Fragestellung die Antwort in einer der *k* - von der Stichwortsuche - gefundenen Dokumente steckt.\n",
    "\n",
    "###  Semantic Textual Similarity (STS)\n",
    "Ein sentence transformer Model mapped eine Texteingabe auf einen 384-dimensionalen Feature-Vektor. Dabei wird die semantische Information kodiert. Verschiedene Vektoren kann man dann mittels ihrer Kosinus-√Ñhnlichkeit vergleichen. Semnatisch √§hnliche Embeddings haben eine h√∂here Kosinus-√Ñhnlichkeit als semantisch verschiedene. \n",
    "\n",
    "Das Verfahren eignet sich f√ºr S√§tze und kurze Paragraphen (256 W√∂rter max). Es k√∂nnte dazu genutzt werden das Embedding einer Query mit den verschiedenen Paragraphen auf √Ñhnlichkeit zu vergleichen.\n",
    "\n",
    "Die Inferenzzeit ist gering, denn wie bei einem Suchindex k√∂nnten f√ºr alle Paragraphen der Doku die Embeddings berechnet und abgespeichert werden. Bei einer Anfrage muss dann nur die Query kodiert und die Kosinus-√Ñhnlichkeit mit allen Paragraphen berechnet werden (kein erneuter Pass durch Model n√∂tig).\n",
    "\n",
    "### (Word2Vec)\n",
    "(Im Grunde die gleiche Idee wir bei STS)\n",
    "\n",
    "https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630\n",
    "\n",
    "\n",
    "### Passage Ranking\n",
    "Passage Ranking Modelle nehmen ein Set an Dokumenten und eine Anfrage Query und geben ein Ranking der Dokumente heraus, wie gut die Query auf alle verf√ºgbaren Dokumente passt [1]. Das k√∂nnte man als simples QA Modell verwenden, indem die Query die Frage und die Dokumente die Knowledge Base darstellen. \n",
    "\n",
    "[1] https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco <br>\n",
    "\n",
    "*Die Modelle / das Vorgehen bei STS und Passage Ranking scheinen sehr √§hnlich zu sein, nur unterscheidet sich der Task leicht. Die Hugginface API nutzt f√ºr STS dieses Modell [1] f√ºr Passage Ranking dieses [2]. Die Benchmark-Datens√§tze sind auch unterschiedlich.*\n",
    "\n",
    "[1] https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 <br>\n",
    "[2] https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b\n",
    "\n",
    "**Alle hier aufgef√ºhrten Verfahren k√∂nnten auch in Kombination mit einem QA-Modell genutzt werden, um die Inferenzzeit zu minimieren, da √ºber ein schnelleres Verfahren eine Vorauswahl an Dokumenten getroffen werden kann. Bei kombinierten Verfahren sind die Ergebnisse aber nicht mehr wirklich vergleichbar.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36567d63",
   "metadata": {},
   "source": [
    "## quantitative G√ºtema√üe\n",
    "\n",
    "### open vs. clodes domain evaluation\n",
    "Bei der **closed domain** evalutaion wird f√ºr jede Antwort einer Frage bezogen auf einen bestimmten Kontext / Dokument neben dem Antwort-String auch der Index des Start- und Endtoken angegeben. Als \"richtig\" beantwortet gilt die Frage durch das QA-System nur dann, wenn nicht nur der String sondern auch die Indizes matchen. Der Squad-Datensatz in allen seinen Variationen ist so annotiert.\n",
    "\n",
    "Bei **open domain** ist f√ºr eine Frage lediglich der Antwort-String annotiert. Als richtig predicted z√§hlt dann jeder String = Antwort-String, egal wo im Dokument die Antwort zu finden ist. Diese Form macht mehr Sinn, wenn man davon ausgehen kann, dass die Antwort auf eine Frage an mehr als einer Stelle in einer Knowledge Base zu finden ist (*Tendenziell bei ECU-Test-Doku der Fall!*) \n",
    "\n",
    "### Retriever Metriken\n",
    "\n",
    "Hier definiert man ein *k* >= 1, das angibt, wieviele m√∂gliche Antwortkanditaten vom Model ausgegeben werden sollen. Diese werden nach ihrem Score ge-ranked.  \n",
    "\n",
    "#### Recall\n",
    "Der Recall auf einem Testdatensatz gibt dann an, wie oft f√ºr *n* Queries das richtige Dokument unter den jeweils *k* besten Antworten enthalten war. Hier kann man dann etwa verschiedene Schwellwerte definieren und somit etwa den **TOP5-Score**, **TOP3-Score** oder **TOP1-Score** berechnen. Auf welcher Position das korrekte Dokument dabei liegt, spielt hier keine Rolle.\n",
    "\n",
    "#### Mean Reciprocal Rank (MRR)\n",
    "Hier wird im Gegensatz zum Recall der Rang mit einbezogen. Hat das gesuchte Dokument von allen *k* Dokumenten die h√∂chste Konfidenze / Score und ist somit auf Platz eins, hat es den Reciprocal Rank (RR) von *1*. Sonst von 1/Rang. Also etwa 1/3 f√ºr den dritten Platz. Gemittelt √ºber alle Queries eines Testdatensatzes ergibt es dann also den MMR mit maximal 1.0.\n",
    "\n",
    "https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_488\n",
    "\n",
    "### Reader Metriken\n",
    "\n",
    "#### Exact Match (EM)\n",
    "F√ºr jedes Frage / Antwort Paar wird die Prediction gegen√ºber der Ground Truth verglichen. Nur bei einer genauen √úberlappung der Text Passage wird die Prediction als ‚ÄúTreffer‚Äù gez√§hlt. Verschiebung um eine Position im Satz, macht die Prediction zu einem Miss.\n",
    "\n",
    "#### F1-Score\n",
    "Hier z√§hlt die Schnittmenge von Prediction und wahrer Antwort. Precision ist dabei die Anzahl der W√∂rter in der Schnittmenge gegen√ºber aller W√∂rter in der Prediction, Recall ist das Verh√§ltnis der W√∂rter in der Schnittmenge zu allen W√∂rtern in der Ground Truth. \n",
    "\n",
    "![equation f1 score](./imgs/f1_score.png)\n",
    "\n",
    "#### Accuracy\n",
    "Antwort (Ground Truth) und Prediction m√ºssen nicht 100% matchen, es reicht etwa wenn die Prediction in der tats√§chlichen Antwort enthalten ist um als richtig gewertet zu werden.\n",
    "\n",
    "### Semantic Answer Similarity\n",
    "Das ist Model-basierte Metric, wobei die semantische √Ñhnlichkeit von Ground Truth und Prediction verglichen wird. Diese Metrik ist somit weniger anf√§llig f√ºr Umschreibungen, Abk√ºrzungen, Ausschreiben von Zahlen, etc.\n",
    "\n",
    "\n",
    "https://www.deepset.ai/blog/semantic-answer-similarity-to-evaluate-qa\n",
    "\n",
    "\n",
    "### Eigene\n",
    "\n",
    "\n",
    "\n",
    "Je nach Task k√∂nnen eigene Metriken definiert werden: Ist die Aufgabe etwa eine Frage nur einem Abschnitt zuzuordnen, ist die genaue Positionierung der Satztokens jedoch irrelevant, k√∂nnte eine Accuracy angegeben werden, in wie vielen F√§llen die Zuordnung zum Richtigen Dokument erfolgte. \n",
    "\n",
    "[\\*] https://www.deepset.ai/blog/how-to-evaluate-a-question-answering-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59def6",
   "metadata": {},
   "source": [
    "## Public Benchmarks /  Welche gibt es Speziell im NLP-Bereich?\n",
    "\n",
    "QA-Modelle werden auf einer Reihe von Datasets gebenchmarkt, vorne dran SQuAD 1 & SQuAD 2, sowie die jeweiligen dev Splits (Validierungsdaten mit negativen Samples und Mehrfachantworten) [1].\n",
    "\n",
    "[2] zeigt etwa eine √úbersicht der Performance der besten Modelle auf dem SQuAD 2 Datensatz.  \n",
    "\n",
    "Die Relevanz f√ºr unser Problem ist aber ein bisschen fraglich, denn die Modelle m√ºssen zum einen vortrainiert verf√ºgbar sein, zum anderen auf deutschen Datens√§tzen finetuned sein. \n",
    "\n",
    "[1] https://paperswithcode.com/task/question-answering <br>\n",
    "[2] https://paperswithcode.com/sota/question-answering-on-squad20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4638d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d01bc42",
   "metadata": {},
   "source": [
    "# ‚ùìFragen Backlog\n",
    "\n",
    "### Qualitativer Verfahrensvergleich auf Problemstellung bezogen\n",
    "\n",
    "### Quantitativer, grunds√§tzlicher Verfahrensvergleich mit Benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0edddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (quaseldoku)",
   "language": "python",
   "name": "kedro_quaseldoku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
